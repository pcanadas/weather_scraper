{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Leer las URLs del archivo\n",
    "file_name = 'data/links_prevision.txt' # Nombre del archivo\n",
    "\n",
    "# Cargar URLs eliminando duplicados\n",
    "with open(file_name, 'r') as file:\n",
    "    urls = {line.strip() for line in file if \"http\" in line}\n",
    "\n",
    "# Imprimir el número de URLs encontradas\n",
    "print(f\"Se encontraron {len(urls)} urls en el archivo {file_name}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar progreso existente\n",
    "hoy = time.strftime(\"%Y-%m-%d\")  # Fecha actual\n",
    "def cargar_progreso():\n",
    "    progreso_file = f'progreso/progreso_prevision_{hoy}.csv' # Archivo para guardar el progreso\n",
    "    try:\n",
    "        procesadas = pd.read_csv(progreso_file, header=None)[0].tolist()\n",
    "        print(f\"Se encontraron {len(procesadas)} URLs procesadas anteriormente\")\n",
    "    except FileNotFoundError:\n",
    "        procesadas = []\n",
    "    return procesadas, progreso_file\n",
    "\n",
    "procesadas, progreso_file = cargar_progreso()\n",
    "\n",
    "# Filtrar las URLs no procesadas\n",
    "urls_restantes = [url for url in urls if url not in procesadas]\n",
    "print(f\"Se encontraron {len(urls_restantes)} URLs restantes para procesar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del navegador\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--window-size=1920,1080')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.110 Safari/537.36\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar cada URL\n",
    "def url_clima(url):\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    try:\n",
    "        # Abre la URL\n",
    "        driver.get(url)\n",
    "        driver.save_screenshot('debug_screenshot.png')\n",
    "\n",
    "        # Intenta cerrar el popup si existe\n",
    "        try:\n",
    "            iframe = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"sp_message_iframe_1200818\"]'))\n",
    "            )\n",
    "            driver.switch_to.frame(iframe)\n",
    "            try:\n",
    "                # Ahora intenta localizar y hacer clic en el botón \"Aceptar\" dentro del iframe\n",
    "                button_accept = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Aceptar\")]'))\n",
    "                )\n",
    "                button_accept.click()\n",
    "                print(\"Botón 'Aceptar' clickeado.\")\n",
    "            except Exception as e:\n",
    "                print(\"El botón 'Aceptar' no se encontró o no fue clickeable.\")\n",
    "            finally:\n",
    "                driver.switch_to.default_content()\n",
    "                print(f\"Popup cerrado\")\n",
    "        except Exception:\n",
    "            print(f\"El iframe del popup no apareció. Continuando...\")\n",
    "            pass\n",
    "\n",
    "        # Extraer el HTML procesado\n",
    "        time.sleep(5)  # Pausa de 5 segundos antes de continuar\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extraer el texto de la cabecera\n",
    "        header = soup.find('div', class_='page-header')\n",
    "        header_text = header.get_text(strip=True) if header else \"Sin título\"\n",
    "\n",
    "        # Añadir fecha de hoy\n",
    "        date_text = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        ciudad = header_text\n",
    "\n",
    "        # Formar el título\n",
    "        title = f\"{header_text} - {date_text}\"\n",
    "\n",
    "        # Verificar si el título contiene \"Error\"\n",
    "        if \"Error\" in title:\n",
    "            now = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"{now} - Se detectó un error en la página. Esperando 5 minutos antes de continuar.\")\n",
    "            driver.quit()  # Cerrar el navegador\n",
    "            time.sleep(300)  # Pausa de 5 minutos antes de continuar\n",
    "            return  # Salir de la función\n",
    "        \n",
    "        print(f\"Título de la página: {title}\")\n",
    "\n",
    "        # Extraer contenido específico de la página\n",
    "        tabla = soup.find('table', class_='table fourteen-table')  # Buscar la tabla con la clase específica\n",
    "\n",
    "        informacion_tabla = []\n",
    "        if tabla:\n",
    "            filas = tabla.find_all('tr')  # Encontrar todas las filas de la tabla\n",
    "            for fila in filas:\n",
    "                celdas = fila.find_all(['td', 'th'])  # Buscar celdas de la fila (pueden ser 'td' o 'th')\n",
    "                fila_datos = [celda.get_text(strip=True) for celda in celdas]  # Extraer el texto de cada celda\n",
    "                if fila_datos:  # Si la fila no está vacía\n",
    "                    informacion_tabla.append(fila_datos)\n",
    "            \n",
    "            print(f\"Información de pronostico extraída\")\n",
    "        else:\n",
    "            print(f\"No se encontró contenido relevante en {url}\")\n",
    "            time.sleep(10)  # Pausa de 10 segundos antes de continuar\n",
    "\n",
    "        # Convertir la información extraída en un DataFrame\n",
    "        if informacion_tabla:\n",
    "            tabla_info = pd.DataFrame(informacion_tabla)\n",
    "            # Si las columnas tienen encabezados, usa la primera fila como nombres de columna\n",
    "            if len(tabla_info) > 1:\n",
    "                tabla_info.columns = tabla_info.iloc[0]  # Usar la primera fila como encabezado\n",
    "                tabla_info = tabla_info[1:]  # Eliminar la fila de encabezados del contenido\n",
    "            tabla_info.reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            tabla_info = pd.DataFrame()\n",
    "\n",
    "        # Agregar información adicional (si aplica)\n",
    "        tabla_info['ciudad'] = ciudad\n",
    "        tabla_info['fecha_datos'] = date_text\n",
    "        tabla_info['url'] = url\n",
    "        # Renombrar la columna específica\n",
    "        if len(tabla_info.columns) > 5:  # Asegurarte de que exista la columna 6\n",
    "            columna_actual = tabla_info.columns[5]\n",
    "            if columna_actual != 'Clima':  # Solo renombrar si no se llama ya 'Clima'\n",
    "                tabla_info.rename(columns={columna_actual: 'Clima'}, inplace=True)\n",
    "\n",
    "        # Eliminar columnas duplicadas si existen\n",
    "        tabla_info = tabla_info.loc[:, ~tabla_info.columns.duplicated()]\n",
    "\n",
    "        # Reordenar columnas: 'ciudad', 'fecha_datos', 'url' primero, y luego el resto\n",
    "        columnas_deseadas = ['ciudad', 'fecha_datos', 'url', 'Fecha', 'Temperatura', 'Clima', 'Viento'] # Columnas en el orden deseado\n",
    "        # Eliminar filas con la fecha 'Más adelante'\n",
    "        tabla_info = tabla_info[tabla_info['Fecha'] != 'Más adelante']\n",
    "        tabla_final = tabla_info[columnas_deseadas]  # Reordenar las columnas\n",
    "\n",
    "        # Nombre del archivo\n",
    "        file_path = f'crudo/pronostico_{date_text}.csv'\n",
    "\n",
    "        # Comprobar si el archivo ya existe\n",
    "        file_exists = os.path.exists(file_path)\n",
    "\n",
    "        # Guardar los datos en el archivo CSV\n",
    "        tabla_final.to_csv(\n",
    "            file_path,\n",
    "            mode='a',  # Modo \"append\" para agregar datos sin sobrescribir\n",
    "            index=False,\n",
    "            header=not file_exists,  # Escribir encabezado solo si el archivo no existe\n",
    "            encoding='utf-8-sig')\n",
    "        print(\"Datos de pronostico guardados\")\n",
    "    \n",
    "        # Registrar progreso\n",
    "        with open(progreso_file, 'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "        print(f\"Progreso guardado\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar la URL {url}: {e}\")\n",
    "    finally:\n",
    "        # Cerrar el navegador para esta URL\n",
    "        driver.quit()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar las URLs \n",
    "for url in urls_restantes:\n",
    "    url_clima(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
