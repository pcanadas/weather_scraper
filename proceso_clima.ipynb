{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Archivos de entrada/salida\n",
    "file_name = 'data/links_tiempo_completo.txt'\n",
    "file_name_act = 'crudo/clima_crudo.csv'\n",
    "progreso_file = 'progreso/progreso_clima.csv'\n",
    "\n",
    "# Cargar URLs eliminando duplicados\n",
    "with open(file_name, 'r') as file:\n",
    "    urls = {line.strip() for line in file if \"http\" in line}\n",
    "\n",
    "print(f\"Se encontraron {len(urls)} URLs en el archivo {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar progreso y filtrar URLs pendientes\n",
    "try:\n",
    "    procesadas = set(pd.read_csv(progreso_file, header=None)[0])\n",
    "except FileNotFoundError:\n",
    "    procesadas = set()\n",
    "\n",
    "urls_restantes = urls - procesadas\n",
    "print(f\"Se encontraron {len(urls_restantes)} URLs pendientes de procesar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del navegador\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--window-size=1920,1080')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.110 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Función para procesar cada URL\n",
    "def url_clima(url):\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.save_screenshot('debug_screenshot.png')\n",
    "\n",
    "        # Intentar cerrar pop-up si existe\n",
    "        try:\n",
    "            iframe = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"sp_message_iframe_1200818\"]'))\n",
    "            )\n",
    "            driver.switch_to.frame(iframe)\n",
    "            button_accept = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Aceptar\")]'))\n",
    "            )\n",
    "            button_accept.click()\n",
    "            print(\"Botón 'Aceptar' clickeado.\")\n",
    "            driver.switch_to.default_content()\n",
    "            print(f\"Popup cerrado\")\n",
    "        except:\n",
    "            print(f\"El iframe del popup no apareció. Continuando...\")\n",
    "            pass  # Si no hay pop-up, continuar normalmente\n",
    "\n",
    "        # Extraer contenido de la página\n",
    "        time.sleep(5)  # Pausa de 5 segundos antes de continuar\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extraer el título principal\n",
    "        header = soup.find('div', class_='page-header')\n",
    "        header_text = header.get_text(strip=True) if header else \"Sin título\"\n",
    "\n",
    "        # Extraer la fecha del sub-header\n",
    "        sub_header = soup.find('div', class_='page-sub-header')\n",
    "        date_span = sub_header.find('span', class_='page-date') if sub_header else None\n",
    "        date_text = date_span.get_text(strip=True) if date_span else \"Sin fecha\"\n",
    "\n",
    "        # Formar el título\n",
    "        title = f\"{header_text} - {date_text}\"\n",
    "\n",
    "        # Verificar si la página tiene error\n",
    "        if \"Error\" in title:\n",
    "            now = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"{now} - Se detectó un error en la página. Esperando 5 minutos antes de continuar.\")\n",
    "            driver.quit()\n",
    "            time.sleep(300)  # Pausa de 5 minutos antes de continuar\n",
    "            return\n",
    "\n",
    "        print(f\"Título de la página: {title}\")\n",
    "\n",
    "        # Extraer datos del clima\n",
    "        clima_divs = soup.find_all('div', class_='month-bubbles past-bubbles')\n",
    "        informacion_clima = []\n",
    "\n",
    "        if clima_divs:\n",
    "            for div in clima_divs:\n",
    "                informacion_clima.extend(div.text.split())  # Extraer y combinar texto de cada div encontrado\n",
    "            print(f\"Información de clima extraída correctamente\")\n",
    "        else:\n",
    "            print(f\"No se encontró contenido relevante en {url}\")\n",
    "            time.sleep(10)  # Pausa de 10 segundos antes de continuar\n",
    "\n",
    "        # Guardar datos en CSV\n",
    "        pd.DataFrame([[url, title, ' '.join(informacion_clima)]]).to_csv(\n",
    "            'crudo/clima_crudo.csv', mode='a', index=False, header=False, encoding='utf-8-sig'\n",
    "        )\n",
    "        print(\"Datos de clima guardados\")\n",
    "\n",
    "        # Registrar progreso\n",
    "        with open(progreso_file, 'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "        print(f\"Procesada: {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {url}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todas las URLs restantes\n",
    "for url in urls_restantes:\n",
    "    url_clima(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados en el archivo de clima procesado\n",
    "pd.read_csv(file_name_act).drop_duplicates().to_csv(file_name_act, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
